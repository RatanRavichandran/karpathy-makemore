{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt# reading the data\n",
        "words = open(\"/content/names.txt\", \"r\").read().splitlines()\n",
        "\n",
        "# Exploring\n",
        "print(f\"first 10 words{words[:10]}\")\n",
        "print(f\"length of words: {len(words)}\")\n",
        "print(f\"min word length {min(len(w) for (w) in words)} and max word length {max(len(w) for (w) in words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS-NOsX9xcVN",
        "outputId": "664b1be2-701a-4a63-bbb2-64b057696911"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 10 words['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
            "length of words: 32033\n",
            "min word length 2 and max word length 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
      ],
      "metadata": {
        "id": "AKRht0umxFoL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4Ov2zERfwaj-"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(\"\".join(words))))\n",
        "chars = [\".\"] + chars\n",
        "\n",
        "stoi = {ch: i for (i, ch) in enumerate(chars)}\n",
        "\n",
        "itos = {i: ch for (ch, i) in stoi.items()}\n",
        "\n",
        "N = torch.ones(27, 27, 27, dtype = torch.int32)\n",
        "N[0, 0, 0] = 0\n",
        "# getting the Bigrams\n",
        "for w in words:\n",
        "    # add start and end tokens\n",
        "    chs = [\".\"] + list(w) + [\".\"]\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        ix3 = stoi[ch3]\n",
        "\n",
        "        N[ix1, ix2, ix3] += 1\n",
        "\n",
        "P = N / N.sum(dim = 2, keepdim = True)\n",
        "\n",
        "def count_loss(input_list, verbose = False):\n",
        "    log_likelihood = 0.0\n",
        "    n = 0\n",
        "    for w in input_list:\n",
        "        chs = [\".\"] + list(w) + [\".\"]\n",
        "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "            ix1 = stoi[ch1]\n",
        "            ix2 = stoi[ch2]\n",
        "            ix3 = stoi[ch3]\n",
        "\n",
        "            prob = P[ix1, ix2, ix3]\n",
        "            logprob = torch.log(prob)\n",
        "            log_likelihood += logprob\n",
        "            n += 1\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"{ch1}{ch2} -> {prob:.4f} {logprob:.4f}\")\n",
        "\n",
        "    # higher the log likelihood (closer to 0) is better\n",
        "    print(f\"log Likelihood: {log_likelihood}\")\n",
        "\n",
        "    # but in loss function lower is better, so we negate it\n",
        "    nll = -log_likelihood\n",
        "    print(f\"Negative log likelihood: {nll}\")\n",
        "\n",
        "    print(f\"Normalized Negative log Likelihood: {(nll / n)}\") # we need to minimize this"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Loss\")\n",
        "count_loss(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soqKoW7yyFYh",
        "outputId": "5bf22caf-80bd-4a21-db85-1d313c972648"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss\n",
            "log Likelihood: -410414.96875\n",
            "Negative log likelihood: 410414.96875\n",
            "Normalized Negative log Likelihood: 2.092747449874878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = []\n",
        "for i in range(10):\n",
        "    out = []\n",
        "    ix1, ix2 = 0, 0\n",
        "    while True:\n",
        "        p = P[ix1, ix2]\n",
        "        ix1 = ix2\n",
        "        ix2 = torch.multinomial(p, 1, replacement=True).item()\n",
        "        if ix2 == 0:\n",
        "            break\n",
        "        out.append(itos[ix2])\n",
        "\n",
        "    names.append(\"\".join(out))\n",
        "\n",
        "print(names)\n",
        "print(\"Sampled words Loss\")\n",
        "count_loss(names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1RqVluUyKVR",
        "outputId": "5b5bb82a-2ff8-44a7-f059-1f67570651c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['di', 'ezrechaira', 'ell', 'apriya', 'bromimaita', 'chawaleignyxon', 'tni', 'khyana', 'uria', 'xais']\n",
            "Sampled words Loss\n",
            "log Likelihood: -131.75120544433594\n",
            "Negative log likelihood: 131.75120544433594\n",
            "Normalized Negative log Likelihood: 2.1250195503234863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
      ],
      "metadata": {
        "id": "LIsoOFmXyS3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "words_train, words_test = train_test_split(words, test_size=0.2, random_state=1234)\n",
        "words_dev, words_test = train_test_split(words_test, test_size=0.5, random_state=1234)\n",
        "\n",
        "x_train, y_train, x_dev, y_dev, x_test, y_test = [], [], [], [], [], []\n",
        "for wgroup in [words_train, words_dev, words_test]:\n",
        "    xs , ys = [], []\n",
        "    for w in wgroup:\n",
        "        # add start and end tokens\n",
        "        chs = [\".\"] + list(w) + [\".\"]\n",
        "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "            ix1 = stoi[ch1]\n",
        "            ix2 = stoi[ch2]\n",
        "            ix3 = stoi[ch3]\n",
        "\n",
        "            xs.append([ix1, ix2])\n",
        "            ys.append(ix3)\n",
        "\n",
        "    xs = torch.tensor(xs, dtype=torch.int64)\n",
        "    ys = torch.tensor(ys, dtype=torch.int64)\n",
        "\n",
        "    if wgroup == words_train:\n",
        "        x_train, y_train = xs, ys\n",
        "    elif wgroup == words_dev:\n",
        "        x_dev, y_dev = xs, ys\n",
        "    else:\n",
        "        x_test, y_test = xs, ys"
      ],
      "metadata": {
        "id": "S-sIFUmOyUbT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27*2,27), requires_grad = True)\n",
        "for k in range(200):\n",
        "    # forward pass\n",
        "    xenc = F.one_hot(x_train, num_classes = 27).float()\n",
        "    xenc = xenc.view(-1, 27*2)\n",
        "\n",
        "    # probs is softmax\n",
        "    logits = xenc @ W\n",
        "    counts = torch.exp(logits)\n",
        "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
        "\n",
        "    # loss (normalized negative log likelihood)\n",
        "    loss = - probs[torch.arange(len(x_train)), y_train].log().mean()\n",
        "    # add regularization\n",
        "    # loss += 0.2 * W.pow(2).mean()\n",
        "\n",
        "    if k % 10 == 0:\n",
        "        print(f\"{k}: {loss.item():.4f}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # update weights\n",
        "    with torch.no_grad():\n",
        "        W -= 50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRW4brhiyjVi",
        "outputId": "4cdfa7da-7931-45fe-8ecb-17db5ac15d67"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 4.3064\n",
            "10: 2.4965\n",
            "20: 2.3742\n",
            "30: 2.3285\n",
            "40: 2.3051\n",
            "50: 2.2911\n",
            "60: 2.2816\n",
            "70: 2.2747\n",
            "80: 2.2695\n",
            "90: 2.2654\n",
            "100: 2.2621\n",
            "110: 2.2595\n",
            "120: 2.2572\n",
            "130: 2.2554\n",
            "140: 2.2538\n",
            "150: 2.2524\n",
            "160: 2.2512\n",
            "170: 2.2502\n",
            "180: 2.2493\n",
            "190: 2.2485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_loss(x, y, W):\n",
        "    xenc = F.one_hot(x, num_classes = 27).float()\n",
        "    xenc = xenc.view(-1, 27*2)\n",
        "\n",
        "    # probs is softmax\n",
        "    logits = xenc @ W\n",
        "    counts = torch.exp(logits)\n",
        "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
        "\n",
        "    # loss (normalized negative log likelihood)\n",
        "    loss = - probs[torch.arange(len(x)), y].log().mean()\n",
        "\n",
        "    return loss.item()\n",
        "print(f\"Train Loss: {MLP_loss(x_train, y_train, W):.4f}\")\n",
        "print(f\"Dev Loss: {MLP_loss(x_dev, y_dev, W):.4f}\")\n",
        "print(f\"Test Loss: {MLP_loss(x_test, y_test, W):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzainnody-4b",
        "outputId": "c2cecd59-f480-4084-96a0-8cf3200b360a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2477\n",
            "Dev Loss: 2.2523\n",
            "Test Loss: 2.2512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n"
      ],
      "metadata": {
        "id": "iM504rSNzz6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27*2,27), requires_grad = True)\n",
        "for k in range(200):\n",
        "    # forward pass\n",
        "    xenc = F.one_hot(x_train, num_classes = 27).float()\n",
        "    xenc = xenc.view(-1, 27*2)\n",
        "\n",
        "    # probs is softmax\n",
        "    logits = xenc @ W\n",
        "    counts = torch.exp(logits)\n",
        "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
        "\n",
        "    # loss (normalized negative log likelihood)\n",
        "    loss = - probs[torch.arange(len(x_train)), y_train].log().mean()\n",
        "    # add regularization\n",
        "    # loss += 0.05 * W.pow(2).mean()\n",
        "\n",
        "    if k % 10 == 0:\n",
        "        print(f\"{k}: Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(x_dev, y_dev, W):.4f}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # update weights\n",
        "    with torch.no_grad():\n",
        "        W -= 50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9ImipQYz4uL",
        "outputId": "b23828ad-e23f-4d09-ff7e-d60a3d52bb3a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: Train Loss: 4.0929 | Dev Loss 4.1041\n",
            "10: Train Loss: 2.4989 | Dev Loss 2.5058\n",
            "20: Train Loss: 2.3767 | Dev Loss 2.3816\n",
            "30: Train Loss: 2.3298 | Dev Loss 2.3344\n",
            "40: Train Loss: 2.3053 | Dev Loss 2.3098\n",
            "50: Train Loss: 2.2905 | Dev Loss 2.2950\n",
            "60: Train Loss: 2.2807 | Dev Loss 2.2852\n",
            "70: Train Loss: 2.2737 | Dev Loss 2.2782\n",
            "80: Train Loss: 2.2684 | Dev Loss 2.2729\n",
            "90: Train Loss: 2.2643 | Dev Loss 2.2689\n",
            "100: Train Loss: 2.2611 | Dev Loss 2.2656\n",
            "110: Train Loss: 2.2584 | Dev Loss 2.2630\n",
            "120: Train Loss: 2.2563 | Dev Loss 2.2608\n",
            "130: Train Loss: 2.2545 | Dev Loss 2.2590\n",
            "140: Train Loss: 2.2529 | Dev Loss 2.2575\n",
            "150: Train Loss: 2.2516 | Dev Loss 2.2563\n",
            "160: Train Loss: 2.2505 | Dev Loss 2.2551\n",
            "170: Train Loss: 2.2495 | Dev Loss 2.2542\n",
            "180: Train Loss: 2.2486 | Dev Loss 2.2533\n",
            "190: Train Loss: 2.2478 | Dev Loss 2.2526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n"
      ],
      "metadata": {
        "id": "5a5OtS1Lz-i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27*2,27), requires_grad = True)\n",
        "for k in range(200):\n",
        "    # forward pass\n",
        "    # ====================\n",
        "    # Previously: using onehot and multiplying by W\n",
        "    # xenc = F.one_hot(xs, num_classes = 27).float().to(device)\n",
        "    # xenc = xenc.view(-1, 27*2)\n",
        "    # logits = xenc @ W\n",
        "    # ====================\n",
        "\n",
        "    # ====================\n",
        "    # ✅ now: acess by xs indices directly\n",
        "    logits = W[xs[:,0]] + W[xs[:,1] + 27]\n",
        "    # ====================\n",
        "\n",
        "    counts = torch.exp(logits)\n",
        "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
        "\n",
        "    # loss (normalized negative log likelihood)\n",
        "    loss = - probs[torch.arange(len(xs)), ys].log().mean()\n",
        "    # add regularization\n",
        "    loss += 0.2 * W.pow(2).mean()\n",
        "\n",
        "    if k % 10 == 0:\n",
        "        print(f\"{k}: {loss.item():.4f}\")\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # update weights\n",
        "    with torch.no_grad():\n",
        "        W -= 50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpr0k3nB0DXg",
        "outputId": "d706f965-3ba7-4f13-812a-9ea378512887"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 4.5407\n",
            "10: 2.5866\n",
            "20: 2.4540\n",
            "30: 2.4065\n",
            "40: 2.3839\n",
            "50: 2.3715\n",
            "60: 2.3640\n",
            "70: 2.3594\n",
            "80: 2.3563\n",
            "90: 2.3543\n",
            "100: 2.3530\n",
            "110: 2.3521\n",
            "120: 2.3514\n",
            "130: 2.3510\n",
            "140: 2.3506\n",
            "150: 2.3504\n",
            "160: 2.3503\n",
            "170: 2.3502\n",
            "180: 2.3501\n",
            "190: 2.3500\n"
          ]
        }
      ]
    }
  ]
}